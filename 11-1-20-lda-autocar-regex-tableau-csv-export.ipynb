{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Warranty Data Analysis Autocar \n## Updated Approach using Regular Expressions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"../input\"))\n\n# Plotly based imports for visualization\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS  #very important to change language\nfrom spacy.lang.en import English\n!python -m spacy download es_core_news_sm\n\n#Pandas display width\npd.set_option('max_colwidth',100) #max width df display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data for Autocar\nwines = pd.read_csv('../input/dbwmlda/dbwmalda2.csv')\nwines.head(10)\nwines.text[15506]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## wines.shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"# *** Regular Expresion Cleaning\nimport re\nimport string\ndef clean_text_round1(text):\n    text=re.sub(\"_x000D_\",'',text)\n    text=re.sub(\"Autocar\",'',text)\n    text=re.sub(\"AUTOCAR\",'',text)\n    text=re.sub(\"retorque\",'',text)\n    text=re.sub(\"RETORQUE\",'',text)\n    text=re.sub(\"Retorque\",'',text)\n    text=re.sub('\\n','',text)\n    text=re.sub('\\(.*?\\)','',text)\n    text=re.sub('WHEELSRIMHUB','',text)\n    text=re.sub('CABSHEET','',text)\n    text=re.sub('ENGINEPOWER','',text)\n    text=re.sub('LIGHTING','',text)\n    text=re.sub('EXPENDABLEITMES','',text)\n    text=re.sub('BRAKE','',text)\n    text=re.sub('COOLSYSTEM','',text)\n    text=re.sub('EXHAUST','',text)\n    text=re.sub('C1:','',text)\n    text=re.sub('C2:','',text)\n    text=re.sub('C3:','',text)\n    text=re.sub('Complaint:','',text)\n    text=re.sub('CHARGING:','',text)\n    text=re.sub('warranty','',text)\n    text=re.sub('Warranty','',text)\n    text=re.sub('WARRANTY','',text)\n    text=re.sub('cancel','',text)\n    text=re.sub('repair','',text)\n    text=re.sub('complaint--','',text)\n    text=re.sub('complaint','',text)\n    text=re.sub(\"[^A-Za-z']+\", ' ',text)\n    text=re.sub('Deferred','',text)\n    text=re.sub('Order','',text)\n    text=re.sub('order','',text)\n    return(text)\n\nround1 = lambda x: clean_text_round1(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean=pd.DataFrame(wines.text.apply(round1)) #wines is the data frame and 'text' is the name of the field bien processed\ndata_clean.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wines.text=data_clean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Registra el numero de temas a identificar en la siguiente línea"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Aqui se define el numero de temas a buscar y el nombre del campo del data frame que contiene la informacion a analizar\ntotal_topics=4\nfield_source='text'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Updating the wine df with the processed query\n#wines=newdf\nwines.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seleccionando campo fuente del dataframe \nwines[field_source][3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a spaCy object\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"677e753450ddea24b1289e3033a2f7928710ef25"},"cell_type":"code","source":"#Verficando que se esta procesando el registro correcto\ndoc = nlp(wines[field_source][3])\nspacy.displacy.render(doc, style='ent',jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25330472382f10be59dd09d3f4cb7a8ae5686081"},"cell_type":"markdown","source":"## Econtrando la raíz de los términos (Lemmatization)\n## Creación de lista de puntuación y palabras comunes que no añaden mucha informaciónCY\n"},{"metadata":{"trusted":true,"_uuid":"3e004fb15995ec7723ca12ad54baf98029ecf90e"},"cell_type":"code","source":"#definicion de puntuacion y STOP WORDS\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n#stopwords.append('wheelsrimhub')# added 'empresa' as a Stop Word\n#stopwords.append('x000d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Quitar signo de comentarios para revisar listas de stopwords y punctuations\n#stopwords\n#punctuations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5c09cd0724ac75d603b3e1be1afb006249eadc4"},"cell_type":"code","source":"review = str(\" \".join([i.lemma_ for i in doc]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868105f99baf2742713656a089ad5b38f80d8329"},"cell_type":"code","source":"doc = nlp(review)\nspacy.displacy.render(doc, style='ent',jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cd922f8229f6aba3f08dfb25d8793a5e2b0ee57"},"cell_type":"markdown","source":"## Detectando los componentes dentro de las frases ###\n\n"},{"metadata":{"trusted":true,"_uuid":"a8ed2a71e2d90ad317e7b0d69d0e6544ff19f17c"},"cell_type":"code","source":"# POS tagging\nfor i in nlp(review):\n    print(i,\"=>\",i.pos_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee0b36f52b552dd72c5fda7808d41db5749491cf"},"cell_type":"code","source":"# Parser para las frases escritas Añadi len word >4 to filter out short words with no meaning in this exercise\nparser = English()\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    #mytokens = [ word.lower_ for word in mytokens ] #*either this line or the next one\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] #***this section keeps only the lemma removed\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations and len(word)>4] #Eliminating short words\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utilizando la variable field_source como la columna a analizar de la base de datos\nspacy_tokenizer(wines[field_source][4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1207bc8f6ca068de633cd8d68d1424877baf85a"},"cell_type":"code","source":"# Applies the parser and filters information to get a processed data frame\n# Important selection on the column of the wines field_source\ntqdm.pandas()\nwines[\"processed_source\"] = wines[field_source].progress_apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wines[\"processed_source\"][0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e64d8d97ef7443b594431dc4662450264460aae1"},"cell_type":"markdown","source":"## Encontrando los temas principales entre todos los documentos\n\n## Creación de un vector de términos\n"},{"metadata":{},"cell_type":"markdown","source":"## Aquí es donde se hace el fit a la vectorizacion de la data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a vectorizer version without the stop words option\nvectorizer = CountVectorizer( lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ndata_vectorized = vectorizer.fit_transform(wines[\"processed_source\"]) #procesando processed source\n#type(data_vectorized)\ndata_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc59a5306eb806598d98f79daabe0217897a5bbe"},"cell_type":"code","source":"NUM_TOPICS = total_topics #parametro desde el principio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f0140ca0b3eb106edeee8afac1720e25eae5675"},"cell_type":"code","source":"# Latent Dirichlet Allocation Model\nlda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True,random_state=42)\ndata_lda = lda.fit_transform(data_vectorized)\nlda.components_.shape\n#type(data_lda)\n#print(data_lda.shape)\n#type(data_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"822195007e669911dffd6162bfc09539f4dd70ee"},"cell_type":"code","source":"# Non-Negative Matrix Factorization Model\n#nmf = NMF(n_components=NUM_TOPICS)\n#data_nmf = nmf.fit_transform(data_vectorized) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c7c3e0f3e59c54f76fbfac0bc1085a81438b25b"},"cell_type":"code","source":"# Latent Semantic Indexing Model using Truncated SVD\n#lsi = TruncatedSVD(n_components=NUM_TOPICS)\n#data_lsi = lsi.fit_transform(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function printing keywords for each topic       *** You may select top terms ***\ndef selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):    #idx and topic become a vector for each iteration through the enumerate command\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i])     #very cleverly algins the names in vectorizer with the topics in model components\n                        for i in topic.argsort()[:-top_n - 1:-1]]) # i here is the index orders in descinding mode keep the first 10 value stop value is -11 to ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a9f1099afa5c6c98ca90103080aa397cd146d3c"},"cell_type":"code","source":"# Keywords for topics clustered by Latent Dirichlet Allocation\nprint(\"LDA Model:\")\nselected_topics(lda, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Habilitar para análsis NMF"},{"metadata":{"trusted":true,"_uuid":"9da862a4cacb585d5ac0e75541c30d8c1add90d1"},"cell_type":"code","source":"# Keywords for topics clustered by Latent Semantic Indexing\n#print(\"NMF Model:\")\n#selected_topics(nmf, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Habilitar para análsis LSI"},{"metadata":{"trusted":true,"_uuid":"e789cfaaf1bca8737ee73f6a6c8a1b388126f13b"},"cell_type":"code","source":"# Keywords for topics clustered by Non-Negative Matrix Factorization\n#print(\"LSI Model:\")\n#selected_topics(lsi, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identificando documentos correspondientes a cada tema"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function printing documents belonging to each topic  *** You may select # of top documents ***\ndef selected_documents(model,wines2,top_n=10):\n    numtemas, tempvar = model.components_.shape\n    print(\"Printing Document belonging to each topic\")\n    #print (doc_topic[ptop_indices]) #Uncomment to see probabilities by topic\n    for idx in range(numtemas): #idx and topic become a vector for each iteration through each topic \n        print(\" \")\n        print(\"Topic %d:\" % (idx))\n        print(\" \")\n        ptop_indices=np.argsort(doc_topic[:,idx])[:-top_n-1:-1] #sorting the indices by the offset column 1 in reverse order, top indices gives the order document\n        print(wines2[ptop_indices])\n        #print (doc_topic[ptop_indices]) #Uncomment to see probabilities by topic\n        if (idx==0):\n            dfcsvtmp=wines2[ptop_indices].to_frame()  #note the use to frame method as the results were just a series\n            dfcsvtmp.insert(1,\"Tema\",idx) #during the first iteration add the column named tema with value idx zero in first iteration\n            print(idx)\n        else:\n            dfcsvtmp2=wines2[ptop_indices].to_frame() #note the use to frame method as the results were just a series\n            dfcsvtmp2.insert(1,\"Tema\",idx) # use a tmp dataframe to add the column named tema and fill it with the current idx iteration\n            dfcsvtmp=dfcsvtmp.append(dfcsvtmp2) # append to previous data frame including the tema column\n    return dfcsvtmp #returning the data frame including all comments and temas\n       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each document calculate the probability for each topic in column form *** VERY IMPORTANT ***\ndoc_topic=lda.transform(data_vectorized) \n#Top documents by topic\n# You can change to the label \"processed_source\" or variable field_source below to see the parsed sentences in the following line\ndfcsv = selected_documents(lda,wines[field_source].str.lower()) #converting results to lower case\ndoc_probs=pd.DataFrame(doc_topic) #creating a dataframe on doc_topic\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcsv\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_probs.iloc[4059,:4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seccion de prueba para una frase"},{"metadata":{"trusted":true},"cell_type":"code","source":"tableau=pd.concat([dfcsv, doc_probs], axis=1, join='inner') #inner joing by index of top n doc with idx and probs ****","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tableau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writing comments with probabilities by topic\n#tableau.to_csv('topicdb_eng.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of transformation of an individual sentence with its probabilities for each topic\ntext = spacy_tokenizer(\"Check engine fial and oil leak\")\nx = lda.transform(vectorizer.transform([text]))[0]\nprint('Probabilities by topic (count start with zero): ',x)\nprint()\nprint(\"El tema con mayor probabilidad usando modelo LDA es el tema #\",np.argmax(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a9eeef90515577592dfebab1635c1f77a7a6868"},"cell_type":"markdown","source":"## Visualizando los Resultado de LDA con la utilería pyLDAvis"},{"metadata":{"trusted":true,"_uuid":"520045555b8cd5ebf89634e580a23f8a221e34c1"},"cell_type":"code","source":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda, data_vectorized, vectorizer, mds='tsne')\ndash","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the LDA visualizaiton model componentes from the dash class just created, coordinates\ncoordinates = pd.DataFrame(dash.topic_coordinates)\ncoordinates.head(10)\ncoordinates.to_csv(\"coordinates_eng.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the LDA visualizaiton model componentes from the dash class just created, Topic Info\ntopic_info=pd.DataFrame(dash.topic_info)\ntopic_info.head(10)\ntopic_info.to_csv('topic_info_eng.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the LDA visualizaiton model componentes from the dash class just created, Token information\ntok_table=pd.DataFrame(dash.token_table)\ntok_table.head(10)\ntok_table.to_csv('tok_table_eng.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pyLDAvis.enable_notebook()\n#dash = pyLDAvis.sklearn.prepare(nmf, data_vectorized, vectorizer, mds='tsne')\n#dash","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb464a9c9c42de1d31dc0bcf4e850cea40af468c"},"cell_type":"markdown","source":"## Cómo interpretar ésta gráfica?\n1. Los temas están a la izquierda y sus palabras respectivas a la derecha.\n2. Los temas más grandes con más frecuents y entre más cercanos más parecidos son.\n3. La selección de las palabras está basada en su capacidad de diferenciación y frecuencia.\n\n**Selecciona el tema para ver sus palabras correspondientes.**"},{"metadata":{"_uuid":"075d5be90085a1d5a197612b5e569768de478b57"},"cell_type":"markdown","source":"## Biagram spaCy tokenizer para la identificación de temas"},{"metadata":{"trusted":true,"_uuid":"2a49363c3f94f7d6a8b40dfd011a8b48d58b7e27"},"cell_type":"code","source":"def spacy_bigram_tokenizer(phrase):\n    doc = parser(phrase) # create spacy object\n    token_not_noun = []\n    notnoun_noun_list = []\n    noun = \"\"\n\n    for item in doc:\n        if item.pos_ != \"NOUN\": # separate nouns and not nouns\n            token_not_noun.append(item.text)\n        if item.pos_ == \"NOUN\":\n            noun = item.text\n        \n        for notnoun in token_not_noun:\n            notnoun_noun_list.append(notnoun + \" \" + noun)\n\n    return \" \".join([i for i in notnoun_noun_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bivectorizer = CountVectorizer(min_df=5, max_df=0.9, lowercase=True, ngram_range=(1,2))\nbigram_vectorized = bivectorizer.fit_transform(wines[\"processed_source\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd6892137ce6f8b2b790d62628d666b53eedb597"},"cell_type":"markdown","source":"## LDA para información procesada con Biagram"},{"metadata":{"trusted":true,"_uuid":"7a2ced631e81c128738660a5d50ae5e5861a54c9"},"cell_type":"code","source":"bi_lda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\ndata_bi_lda = bi_lda.fit_transform(bigram_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f2f160b2983dfff02801e3562af163710f10f6"},"cell_type":"markdown","source":"### Temas pare el modelo de bigram "},{"metadata":{"trusted":true,"_uuid":"607d1d2e5eb51d585eb5abaac8fa2cf4d6430647"},"cell_type":"code","source":"print(\"Bi-LDA Model:\")\nselected_topics(bi_lda, bivectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa5db271862a9d6ca236111867c97f3dcee924d1"},"cell_type":"code","source":"bi_dash = pyLDAvis.sklearn.prepare(bi_lda, bigram_vectorized, bivectorizer, mds='tsne')\nbi_dash","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmaster_list = [['cat', 123, 'yellow'], ['dog', 12345, 'green'], ['horse', 123456, 'red']]\ndf = pd.DataFrame(master_list)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.colheader_justify','light', 'display.width', 2000, 'display.max_colwidth', 500):\n    df = df.stack().str.lstrip().unstack()\n    df = df.style.set_properties(**{'text-align': 'left'})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}